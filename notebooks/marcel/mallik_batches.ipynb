{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from city_sustainability.models import unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Concatenate,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "def encoder(inputs):\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Dropout(0.1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Dropout(0.1)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    return conv1, conv2, pool2\n",
    "\n",
    "\n",
    "def decoder(conv1, conv2, encoded):\n",
    "    up1 = UpSampling2D(size=(2, 2))(encoded)\n",
    "    merge1 = Concatenate(axis=3)([conv2, up1])\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge1)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Dropout(0.1)(conv3)\n",
    "\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    merge2 = Concatenate(axis=3)([conv1, up2])\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge2)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Dropout(0.1)(conv4)\n",
    "\n",
    "    return conv4\n",
    "\n",
    "\n",
    "def build_model(input_shape=(28, 28, 1), num_classes=12):\n",
    "    inputs = Input(input_shape)\n",
    "    conv1, conv2, encoded = encoder(inputs)\n",
    "    decoded = decoder(conv1, conv2, encoded)\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(decoded)\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "def build_vgg16_model(input_shape=(28, 28, 1), num_classes=12 ):\n",
    "    model_vgg = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    for layer in model_vgg.layers:\n",
    "        layer.trainable = False\n",
    "    inputs = model_vgg.layers[0].output\n",
    "    conv1 = model_vgg.get_layer('block1_conv2').output\n",
    "    conv2 = model_vgg.get_layer('block2_conv2').output\n",
    "    encoded = model_vgg.get_layer('block3_conv3').output\n",
    "    decoded = decoder(conv1, conv2, encoded)\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(decoded)\n",
    "    return Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compile_model(model, optimizer='adam'):\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy', compute_iou])\n",
    "def train_model(model, train_generator, epochs=1, steps_per_epoch=None, validation_data=None, validation_steps=None):\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "    early_stopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    model.fit(train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "              validation_data=validation_data, validation_steps=validation_steps,\n",
    "              callbacks=[lr_reducer, early_stopper])\n",
    "\n",
    "\n",
    "def compute_iou(y_true, y_pred):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1, 2, 3]) - intersection\n",
    "    iou = tf.reduce_mean((intersection + 1e-7) / (union + 1e-7))\n",
    "    return iou\n",
    "\n",
    "\n",
    "# def train_model(model, train_generator, epochs=1, batch_size=32, validation_split=0.1):\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy', compute_iou])\n",
    "#     lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "#     early_stopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "#     model.fit(train_generator, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[lr_reducer, early_stopper])\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, x, y):\n",
    "    loss, accuracy = model.evaluate(x, y)\n",
    "    \n",
    "    # Calculate IoU\n",
    "    y_pred = model.predict(x)\n",
    "    iou = compute_iou(y, y_pred)\n",
    "    \n",
    "    print(\"Evaluation results:\")\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, x):\n",
    "    return model.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2303 images belonging to 1 classes.\n",
      "Found 2303 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set the directory paths for images and labels\n",
    "image_directory = '/home/mgudipati/code/Zubairslb/city_sustainability/raw_data/resize_train_all/images'\n",
    "label_directory = '/home/mgudipati/code/Zubairslb/city_sustainability/raw_data/resize_train_all/labels'\n",
    "\n",
    "# Set the target image size\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Set the batch size and number of classes\n",
    "batch_size = 16\n",
    "num_classes = 9\n",
    "\n",
    "# Create an ImageDataGenerator for image augmentation and scaling\n",
    "image_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0  # Scale pixel values between 0 and 1\n",
    ")\n",
    "\n",
    "# Create the image generator\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    target_size=target_size,\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # Do not load class labels initially\n",
    "    seed=42,  # Set a seed for reproducibility\n",
    "    shuffle=True  # Shuffle the order of the images\n",
    ")\n",
    "\n",
    "# Create the label generator\n",
    "label_generator = image_datagen.flow_from_directory(\n",
    "    label_directory,\n",
    "    target_size=target_size,\n",
    "    color_mode='grayscale',  # Load label images in grayscale\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,  # Do not load class labels initially\n",
    "    seed=42,  # Set a seed for reproducibility\n",
    "    shuffle=True  # Shuffle the order of the images\n",
    ")\n",
    "\n",
    "# Combine the image and label generators using zip\n",
    "train_generator = zip(image_generator, label_generator)\n",
    "\n",
    "# Convert labels to one-hot encoding using to_categorical\n",
    "def preprocess_labels(labels):\n",
    "    categorical_labels = to_categorical(labels, num_classes=num_classes)\n",
    "    return categorical_labels\n",
    "\n",
    "# # Apply one-hot encoding to the labels\n",
    "train_generator = ((images, preprocess_labels(labels)) for images, labels in train_generator)\n",
    "# train_generato__r = ((images, labels) for images, labels in train_generator)\n",
    "\n",
    "\n",
    "# Load a sample TIFF image for testing\n",
    "# sample_tiff_path = '../resize_train_all/images/aachen_1.tif'\n",
    "# sample_tiff_image = imageio.imread(sample_tiff_path)\n",
    "# sample_tiff_array = np.array(sample_tiff_image)\n",
    "\n",
    "# Print the shape and data type of the loaded TIFF image\n",
    "# print(\"Sample TIFF image shape:\", sample_tiff_array.shape)\n",
    "# print(\"Sample TIFF image data type:\", sample_tiff_array.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.43137258 0.35686275 0.3647059 ]\n",
      "  [0.4156863  0.3372549  0.34117648]\n",
      "  [0.41960788 0.3254902  0.31764707]\n",
      "  ...\n",
      "  [0.64705884 0.6392157  0.6509804 ]\n",
      "  [0.7372549  0.72156864 0.7176471 ]\n",
      "  [0.7372549  0.72156864 0.7254902 ]]\n",
      "\n",
      " [[0.56078434 0.47450984 0.48235297]\n",
      "  [0.45882356 0.3372549  0.31764707]\n",
      "  [0.42352945 0.32941177 0.32941177]\n",
      "  ...\n",
      "  [0.5176471  0.4666667  0.43529415]\n",
      "  [0.60784316 0.60784316 0.6156863 ]\n",
      "  [0.7254902  0.70980394 0.7058824 ]]\n",
      "\n",
      " [[0.43137258 0.32941177 0.33333334]\n",
      "  [0.4431373  0.29411766 0.25882354]\n",
      "  [0.43137258 0.33333334 0.34901962]\n",
      "  ...\n",
      "  [0.57254905 0.4039216  0.2784314 ]\n",
      "  [0.7607844  0.72156864 0.7137255 ]\n",
      "  [0.70980394 0.7019608  0.7058824 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.65882355 0.6156863  0.5921569 ]\n",
      "  [0.5921569  0.56078434 0.54901963]\n",
      "  [0.63529414 0.6156863  0.6039216 ]\n",
      "  ...\n",
      "  [0.6392157  0.48627454 0.45882356]\n",
      "  [0.61960787 0.5411765  0.5137255 ]\n",
      "  [0.7411765  0.74509805 0.72156864]]\n",
      "\n",
      " [[0.65882355 0.6156863  0.5921569 ]\n",
      "  [0.6392157  0.60784316 0.59607846]\n",
      "  [0.60784316 0.5882353  0.5764706 ]\n",
      "  ...\n",
      "  [0.67058825 0.57254905 0.48627454]\n",
      "  [0.627451   0.5372549  0.4666667 ]\n",
      "  [0.6509804  0.56078434 0.5058824 ]]\n",
      "\n",
      " [[0.68235296 0.6392157  0.6156863 ]\n",
      "  [0.6745098  0.6431373  0.6313726 ]\n",
      "  [0.57254905 0.5529412  0.5411765 ]\n",
      "  ...\n",
      "  [0.63529414 0.5686275  0.49803925]\n",
      "  [0.6117647  0.54509807 0.4666667 ]\n",
      "  [0.6117647  0.54901963 0.45882356]]]\n"
     ]
    }
   ],
   "source": [
    "for x in train_generator:\n",
    "    print(x[0][1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'generator'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m compile_model(model)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train_model(model, train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[38], line 92\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_generator, epochs, batch_size, validation_split)\u001b[0m\n\u001b[1;32m     90\u001b[0m lr_reducer \u001b[39m=\u001b[39m ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m early_stopper \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_generator, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, validation_split\u001b[39m=\u001b[39;49mvalidation_split, callbacks\u001b[39m=\u001b[39;49m[lr_reducer, early_stopper])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/city_sustainability/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/city_sustainability/lib/python3.8/site-packages/keras/engine/data_adapter.py:1668\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1666\u001b[0m unsplitable \u001b[39m=\u001b[39m [\u001b[39mtype\u001b[39m(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _can_split(t)]\n\u001b[1;32m   1667\u001b[0m \u001b[39mif\u001b[39;00m unsplitable:\n\u001b[0;32m-> 1668\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1669\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` is only supported for Tensors or NumPy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1670\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marrays, found following types in the input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(unsplitable)\n\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1673\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays):\n\u001b[1;32m   1674\u001b[0m     \u001b[39mreturn\u001b[39;00m arrays, arrays\n",
      "\u001b[0;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'generator'>]"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = build_vgg16_model(input_shape=(256, 256, 3), num_classes=9)\n",
    "\n",
    "# Compile the model\n",
    "compile_model(model)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_generator, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1843 images belonging to 1 classes.\n",
      "Found 1843 images belonging to 1 classes.\n",
      "Found 460 images belonging to 1 classes.\n",
      "Found 460 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "image_directory = '/home/mgudipati/code/Zubairslb/city_sustainability/raw_data/resize_train_all/images'\n",
    "label_directory = '/home/mgudipati/code/Zubairslb/city_sustainability/raw_data/resize_train_all/labels'\n",
    "\n",
    "image_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "mask_datagen = ImageDataGenerator(validation_split=0.2)\n",
    "\n",
    "seed = 123\n",
    "\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    color_mode='rgb',\n",
    "    target_size=(256, 256),\n",
    "    subset = 'training',\n",
    "    shuffle=False)\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    label_directory,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(256, 256),\n",
    "    subset = 'training',\n",
    "    shuffle=False\n",
    "   )\n",
    "\n",
    "val_image_generator = image_datagen.flow_from_directory(\n",
    "    image_directory,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    color_mode='rgb',\n",
    "    target_size=(256, 256),\n",
    "    subset = 'validation',\n",
    "    shuffle=False)\n",
    "\n",
    "val_mask_generator = mask_datagen.flow_from_directory(\n",
    "    label_directory,\n",
    "    class_mode=None,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    color_mode='grayscale',\n",
    "    target_size=(256, 256),\n",
    "    subset = 'validation',\n",
    "    shuffle=False\n",
    "   )\n",
    "\n",
    "\n",
    "train_generator = zip(image_generator, mask_generator)\n",
    "val_generator = zip(val_image_generator, val_mask_generator)\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    categorical_labels = to_categorical(labels, num_classes=num_classes)\n",
    "    return categorical_labels\n",
    "\n",
    "train_generator = ((images, preprocess_labels(labels)) for images, labels in train_generator)\n",
    "val_generator = ((images, preprocess_labels(labels)) for images, labels in val_generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 18:40:03.509664: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - ETA: 0s - loss: 2.1224 - accuracy: 0.2640 - compute_iou: 0.1003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 18:50:40.797094: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 692s 6s/step - loss: 2.1224 - accuracy: 0.2640 - compute_iou: 0.1003 - val_loss: 1.9304 - val_accuracy: 0.3405 - val_compute_iou: 0.1546 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 674s 6s/step - loss: 1.8143 - accuracy: 0.3636 - compute_iou: 0.1346 - val_loss: 1.6035 - val_accuracy: 0.4176 - val_compute_iou: 0.1611 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 677s 6s/step - loss: 1.7196 - accuracy: 0.3976 - compute_iou: 0.1514 - val_loss: 1.5402 - val_accuracy: 0.4488 - val_compute_iou: 0.1678 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      " 72/115 [=================>............] - ETA: 3:55 - loss: 1.6394 - accuracy: 0.4234 - compute_iou: 0.1656"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = get_model(img_hieght, img_width, img_channel, num_classes)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "# history = model.fit(train_generator,\n",
    "#                     verbose=1,\n",
    "#                     epochs=100,\n",
    "#                     steps_per_epoch= image_generator.samples//batch_size,\n",
    "#                     validation_data= val_generator,\n",
    "#                     validation_steps= val_image_generator.samples//batch_size\n",
    "#                     )\n",
    "\n",
    "# Create the model\n",
    "model = build_vgg16_model(input_shape=(256, 256, 3), num_classes=9)\n",
    "\n",
    "# Compile the model\n",
    "compile_model(model)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_generator,\n",
    "                    epochs=10,\n",
    "                    steps_per_epoch= image_generator.samples//batch_size,\n",
    "                    validation_data= val_generator,\n",
    "                    validation_steps= val_image_generator.samples//batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
